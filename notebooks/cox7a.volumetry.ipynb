{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a *heavily* adapted version of `Zebrafish-Muscles-Volumetry-5268_5269.ipynb`, specifically for the talk at the Institute seminar in 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import imageio\n",
    "import numpy\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask.array\n",
    "import dask_image.imread\n",
    "import dask_image.ndfilters\n",
    "from numcodecs import Blosc\n",
    "import skimage\n",
    "import statsmodels\n",
    "import scipy.signal\n",
    "import sklearn.cluster\n",
    "from skimage.segmentation import random_walker\n",
    "from skimage.filters import threshold_multiotsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\')\n",
    "dask.config.set({'temporary_directory': os.path.join(tmp, 'tmp')})\n",
    "print('Dask temporary files to to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('You can seee what DASK is doing at \"http://localhost:%s/status\"' % client.scheduler_info()['services']['dashboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults for the talk\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "plt.rcParams['savefig.dpi'] = 200\n",
    "plt.rcParams['savefig.transparent'] = True\n",
    "# Set seaborn theme\n",
    "seaborn.set_theme(\n",
    "    context='talk',\n",
    "    style='whitegrid',\n",
    ")\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 2\n",
    "# And then do something like\n",
    "# plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Root = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD', 'Zebrafish_Carolina_Muscles')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory for output\n",
    "# For these notebooks, we simply dump the images one directory up into the relevant 'media' folder\n",
    "OutPutDir = os.path.join('..', 'media', 'cox7a')\n",
    "print('We are saving all the output to %s' % OutPutDir)\n",
    "os.makedirs(OutPutDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the data that we saved from the original notebook, so we don't have to calculate everything again\n",
    "# Data = pandas.DataFrame()\n",
    "Data = pandas.read_pickle(os.path.join(Root, 'Data.Seminnar.2024.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine paths\n",
    "Data['LogFile'] = [log.replace('\\\\', os.path.sep).replace('F:', os.path.split(Root)[0]) for log in Data.LogFile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We found %s subfolders in %s' % (len(Data), Root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Sample'] = [l[len(Root) + 1:].split(os.sep)[0] for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We did several scans for certain samples.\n",
    "# Grab the name of those here\n",
    "Data['RecFolder'] = [os.path.basename(os.path.dirname(l)) for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorbyexperiment(i):\n",
    "    '''Redefine colors for talk'''\n",
    "    if '5268' in i:\n",
    "        return \"#00E6B8\"  # ubRed complimentary\n",
    "        # return (0,230,184)\n",
    "    if '5269' in i:\n",
    "        return \"#E6002E\" # ubRed\n",
    "        # return (230,0,46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Color'] = [colorbyexperiment(f) for f in Data['Sample']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whichfish(i):\n",
    "    '''Label each fish '''\n",
    "    return str(i[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Fish'] = [whichfish(f) for f in Data['Sample']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names of the reconstructions\n",
    "Data['Reconstructions'] = [sorted(glob.glob(os.path.join(f, '*rec0*.png'))) for f in Data['Folder']]\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data.Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reconstructions from the zarr files\n",
    "# If this does not work first run the -Preview notebook!\n",
    "Data['OutputNameRec'] = [((os.path.join(os.path.dirname(f), s)) + '.' + rf + '.zarr') for f, s, rf in zip(Data['Folder'],\n",
    "                                                                                                          Data['Sample'],\n",
    "                                                                                                          Data['RecFolder'])]\n",
    "Reconstructions = [dask.array.from_zarr(file) for file in Data['OutputNameRec']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop RGB axis\n",
    "# See https://dask.discourse.group/t/dask-image-imread-imread-png-as-8bit-not-rgb/2345/\n",
    "Reconstructions = [rec[:,:,:,0] for rec in Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big are the datasets?\n",
    "Data['Size'] = [rec.shape for rec in Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three cardinal directions\n",
    "# Names adapted to fishes: https://en.wikipedia.org/wiki/Fish_anatomy#Body\n",
    "directions = ['Anteroposterior',\n",
    "              'Lateral',\n",
    "              'Dorsoventral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read or calculate the middle slices, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Mid_' + direction] = ''\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.Center.%s.png' % (row['Sample'], row['RecFolder'], direction))\n",
    "        if not os.path.exists(outfilepath):\n",
    "            # Generate requested axial view\n",
    "            if 'Anteroposterior' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][Data['Size'][c][0] // 2].compute()\n",
    "            if 'Lateral' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][:, Data['Size'][c][1] // 2, :].compute()\n",
    "            if 'Dorsoventral' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][:, :, Data['Size'][c][2] // 2].compute()\n",
    "            # Save the calculated 'direction' view to disk\n",
    "            imageio.imwrite(outfilepath, (Data.at[c, 'Mid_' + direction]))\n",
    "        Data.at[c, 'Mid_' + direction] = dask_image.imread.imread(outfilepath).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_' + direction] = ''\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']), '%s.%s.MIP.%s.png' % (row['Sample'], row['RecFolder'], direction))\n",
    "        if not os.path.exists(outfilepath):\n",
    "            # Generate MIP\n",
    "            mip = Data.at[c, 'MIP_' + direction] = Reconstructions[c].max(axis=d).compute()\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, mip)\n",
    "        Data.at[c, 'MIP_' + direction] = dask_image.imread.imread(outfilepath).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose images, so we can view the fish horizontally\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions[1:]):\n",
    "        Data.at[c, 'Mid_' + direction] = Data.at[c, 'Mid_' + direction].transpose()\n",
    "        Data.at[c, 'MIP_' + direction] = Data.at[c, 'MIP_' + direction].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oto(curve, verbose=False):\n",
    "    '''\n",
    "    Function to detect the start/end of the otoliths.\n",
    "    Adapted from the 'detect_minima' function from the ZMK tooth cohort notebook (https://git.io/J3qqL)\n",
    "    Ultimately based on https://stackoverflow.com/a/28541805/323100 and some manual tweaking\n",
    "    '''\n",
    "    from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "    # Smooth the curve and look for the largest deviation\n",
    "    smoothed = lowess(curve, range(len(curve)), return_sorted=False, frac=0.025)\n",
    "    maxima = numpy.argmax(smoothed)\n",
    "    if verbose:\n",
    "        plt.plot(curve, alpha=0.6, label='Input curve')\n",
    "        plt.plot(smoothed, label='LOWESS')\n",
    "        plt.axvline(maxima, c='r', label='Maximum deviation')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return(maxima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headcutter(whichone, sigma=5, threshold=150, verbose=False):\n",
    "    '''\n",
    "    Function to detect where the head is.\n",
    "    We simply look for peaks in the gray values :)\n",
    "    '''\n",
    "    img = Data['MIP_Dorsoventral'][whichone]\n",
    "    # Smooth image for less noise\n",
    "    smoothed = scipy.ndimage.gaussian_filter(img, sigma=sigma, order=0)\n",
    "    # Project average brightness\n",
    "    x = numpy.mean(smoothed > threshold, axis=0)\n",
    "    # Use only the tail-part of the fish\n",
    "    cut = get_oto(x)\n",
    "    if verbose:\n",
    "        plt.imshow(img)\n",
    "        plt.plot(img.shape[0] / x.max() * 0.618 * x, c='r')\n",
    "        plt.axvline(cut)\n",
    "        plt.axis('off')\n",
    "        plt.title(os.path.join(Data.Sample[whichone], Data.RecFolder[whichone]))\n",
    "        outfilepath = os.path.join(os.path.dirname(Data['Folder'][whichone]), '%s.%s.Cut.Head.png' % (Data['Sample'][whichone],\n",
    "                                                                                                      Data['RecFolder'][whichone]))\n",
    "        if not os.path.exists(outfilepath):\n",
    "            plt.savefig(outfilepath, transparent=True, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    return(cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate where we crop the head off\n",
    "Data['HeadCrop'] = [headcutter(i) for i in range(len(Data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minimum(curve, verbose=False):\n",
    "    '''\n",
    "    Function to detect the start/end of the tailfin.\n",
    "    Adapted from the 'detect_minima' function from the ZMK tooth cohort notebook (https://git.io/J3qqL)\n",
    "    Ultimately based on https://stackoverflow.com/a/28541805/323100 and some manual tweaking\n",
    "    '''\n",
    "    from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "    # Smooth the curve and look for the largest deviation\n",
    "    smoothed = lowess(curve, range(len(curve)), return_sorted=False, frac=0.05)\n",
    "    maxima = numpy.argmax(numpy.diff(smoothed))\n",
    "    if verbose:\n",
    "        plt.plot(curve, alpha=0.6, label='Input curve')\n",
    "        plt.plot(smoothed, label='LOWESS')\n",
    "        plt.axvline(maxima, c='r', label='Maximum deviation')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return(maxima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tailcutter(whichone, sigma=5, lookbackwards = 2000, verbose=False):\n",
    "    '''\n",
    "    Function to detect where the tail is.\n",
    "    We simply look for peaks in the gray values :)\n",
    "    '''\n",
    "    img = Data['MIP_Lateral'][whichone]\n",
    "    # Smooth image for less noise\n",
    "    smoothed = scipy.ndimage.gaussian_filter(img, sigma=sigma, order=0)\n",
    "    # Project average brightness\n",
    "    x = numpy.sum(smoothed, axis=0)\n",
    "    # Use only the last 500 slices of the tail-part of the fish\n",
    "    start = Data['Size'][whichone][0] - lookbackwards\n",
    "    cut = get_minimum(x[start:])\n",
    "    if verbose:\n",
    "        plt.imshow(img)\n",
    "        plt.plot(img.shape[0] / x.max() * 0.618 * x, c='r')\n",
    "        plt.axvline(cut + start, c='red')\n",
    "        # plt.axvline(start, c='green')\n",
    "        plt.axis('off')\n",
    "        plt.title(os.path.join(Data.Sample[whichone], Data.RecFolder[whichone]))\n",
    "        outfilepath = os.path.join(os.path.dirname(Data['Folder'][whichone]), '%s.%s.Cut.Tail.png' % (Data['Sample'][whichone],\n",
    "                                                                                                      Data['RecFolder'][whichone]))\n",
    "        if not os.path.exists(outfilepath):\n",
    "            plt.savefig(outfilepath, transparent=True, bbox_inches='tight')       \n",
    "        plt.show()\n",
    "    return(cut + start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate where we crop the tail off\n",
    "Data['TailCrop'] = [tailcutter(i) for i in range(len(Data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 3\n",
    "# And then do something like\n",
    "# plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the locations of the head and tail crop locations\n",
    "for c, row in Data.iterrows():\n",
    "    # plt.subplot(lines, int(numpy.ceil(len(Data) / lines)), c + 1)\n",
    "    plt.imshow(row['MIP_Lateral'])\n",
    "    plt.axvline(row.HeadCrop, c=row.Color)\n",
    "    plt.axvline(row.TailCrop, c=row.Color)\n",
    "    plt.title(row.Sample)\n",
    "    plt.axis('off')\n",
    "    plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OutPutDir, 'cut.%s.png' % row.Sample), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually crop the reconstructions down\n",
    "ReconstructionsCrop = [rec[headcrop:tailcrop] for rec, headcrop, tailcrop in zip(Reconstructions,\n",
    "                                                                                 Data['HeadCrop'],\n",
    "                                                                                 Data['TailCrop'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show some slices laterally along the fishes\n",
    "# for c, rec in enumerate(ReconstructionsCrop):\n",
    "#     for k, i in enumerate(range(50, rec.shape[1], 100)):\n",
    "#         plt.subplot(1, len(range(50, rec.shape[1], 100)), k + 1)\n",
    "#         plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))\n",
    "#         plt.imshow(rec[:,i,:], vmax=150)\n",
    "#         plt.axis('off')\n",
    "#     #     plt.title('%s: slice %s' % (Data['Sample'][c], i))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show some slices laterally along the fishes\n",
    "# for c, rec in enumerate(ReconstructionsCrop):\n",
    "#     for k, i in enumerate(range(50, rec.shape[1], 100)):\n",
    "#         plt.subplot(1, len(range(50, rec.shape[1], 100)), k + 1)\n",
    "#         plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))\n",
    "#         plt.imshow(rec[:,i,:], vmax=150)\n",
    "#         plt.axis('off')\n",
    "#         plt.title('%s: slice %s' % (Data['Sample'][c], i))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate the size of the datasets\n",
    "Data['SizeCrop'] = [rec.shape for rec in ReconstructionsCrop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the cropped middle slices, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Mid_Crop_' + direction] = ''\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.Crop.Center.%s.png' % (row['Sample'], row['RecFolder'], direction))\n",
    "    if os.path.exists(outfilepath):\n",
    "        Data.at[c, 'Mid_Crop_' + direction] = imageio.imread(outfilepath)\n",
    "    else:\n",
    "        # Generate requested axial view\n",
    "        if 'Anteroposterior' in direction:\n",
    "            Data.at[c, 'Mid_Crop_' + direction] = ReconstructionsCrop[c][Data['SizeCrop'][c][0] // 2]\n",
    "        if 'Lateral' in direction:\n",
    "            Data.at[c, 'Mid_Crop_' + direction] = ReconstructionsCrop[c][:, Data['SizeCrop'][c][1] // 2, :]\n",
    "        if 'Dorsoventral' in direction:\n",
    "            Data.at[c, 'Mid_Crop_' + direction] = ReconstructionsCrop[c][:, :, Data['SizeCrop'][c][2] // 2]\n",
    "        # Save the calculated 'direction' view to disk\n",
    "        imageio.imwrite(outfilepath, (Data.at[c, 'Mid_Crop_' + direction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the cropped directional MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_Crop_' + direction] = ''\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']), '%s.%s.Crop.MIP.%s.png' % (row['Sample'], row['RecFolder'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'MIP_Crop_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate MIP\n",
    "            Data.at[c, 'MIP_Crop_' + direction] = ReconstructionsCrop[c].max(axis=-d)\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, Data.at[c, 'MIP_Crop_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show MIP slices\n",
    "# for c, row in Data.iterrows():\n",
    "#     outfilepath = os.path.join(os.path.dirname(row['Folder']), row['Sample'] + '.%s.Crop.MIPs.png' % os.path.basename(row['Folder']))\n",
    "#     if not os.path.exists(outfilepath):\n",
    "#         for d, direction in enumerate(directions):\n",
    "#             plt.subplot(1, 3, d + 1)\n",
    "#             # Push contrast for display\n",
    "#             plt.imshow(skimage.exposure.equalize_adapthist(row['MIP_Crop_' + direction].squeeze()))            \n",
    "#             plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#             plt.title('%s\\n%s' % (os.path.join(row['Sample'], row['RecFolder']),\n",
    "#                                   direction + ' MIP'))\n",
    "#             plt.axis('off')\n",
    "#         plt.savefig(outfilepath, bbox_inches='tight')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the histograms of all (cropped) reconstructions\n",
    "# # Caveat dask.da.histogram returns histogram AND bins, making each histogram a 'nested' list of [h, b]\n",
    "# Data['Histogram'] = [dask.array.histogram(rec, bins=2**8, range=[0, 2**8]) for rec in ReconstructionsCrop]\n",
    "# # Calculate histogram data and put only h into the dataframe, since we use it quite often below.\n",
    "# # Discard the bins\n",
    "# Data['Histogram'] = [h.compute() for h, b in Data['Histogram']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Histogram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def legend_without_duplicate_labels(ax):\n",
    "    '''https://stackoverflow.com/a/56253636/323100'''\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    unique = [(h, l) for i, (h, l) in enumerate(zip(handles, labels)) if l not in labels[:i]]\n",
    "    ax.legend(*zip(*unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, row in Data.iterrows():\n",
    "    plt.plot(row.Histogram, c=row.Color, alpha=.618, label=row.Experiment)\n",
    "legend_without_duplicate_labels(plt.gca())\n",
    "plt.ylim([0,1.5e9])\n",
    "seaborn.despine()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OutPutDir, 'grayvaluehistogram.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, row in Data.iterrows():\n",
    "    plt.semilogy(row.Histogram, c=row.Color, alpha=.618, label=row.Experiment)\n",
    "legend_without_duplicate_labels(plt.gca())\n",
    "plt.ylim([1,1.5e9])\n",
    "seaborn.despine()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OutPutDir, 'grayvaluehistogram.log.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def histogramclustererKmeans(img, number_of_clusters = 5, verbose=False):\n",
    "#     '''Calculate the k-means clusters\n",
    "#     Speed things up with MiniBatchKMeans\n",
    "#     https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html\n",
    "#     '''\n",
    "#     # Setup k-means\n",
    "#     kmeans_volume_subset = sklearn.cluster.MiniBatchKMeans(number_of_clusters)\n",
    "#     # Cluster the histogram into the requested numer of clusters\n",
    "#     # Do this on a subset of the images, to speed things up\n",
    "#     ClusteredImg = kmeans_volume_subset.fit_predict(sorted(numpy.array(img).reshape(-1,1)))\n",
    "#     # Reshape image\n",
    "#     ClusteredImg.shape = img.shape\n",
    "#     if verbose:\n",
    "#         # Calculate histogram\n",
    "#         histogram, bins = dask.array.histogram(img, bins=2**8, range=[0, 2**8])\n",
    "#         plt.semilogy(numpy.log(histogram), label='Gray value histogram')\n",
    "#         plt.semilogy(histogram, label='Gray value histogram (log)')\n",
    "#         for c, cluster in enumerate(sorted(kmeans_volume_subset.cluster_centers_.squeeze())):\n",
    "#             plt.axvline(cluster, label='Cluster center %s at %0.0f' % (c,  cluster),\n",
    "#                         color=seaborn.color_palette(n_colors=number_of_clusters)[c])\n",
    "#         plt.legend()\n",
    "#         plt.xlim([0,2**8])\n",
    "#         plt.title('Logarithmic histogram of input image with %s cluster centers' % number_of_clusters)\n",
    "#         plt.show()\n",
    "#     return(sorted(kmeans_volume_subset.cluster_centers_.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def histogramclustererOtsu(img, number_of_clusters = 5, verbose=False):\n",
    "#     '''Use Multi-Otsu Thresholding instead of k-means clusters'''\n",
    "#     # Applying multi-Otsu threshold with the selected number of classes\n",
    "#     thresholds = threshold_multiotsu(img.compute(), classes=number_of_clusters)\n",
    "#     if verbose:\n",
    "#         # Calculate histogram\n",
    "#         histogram, bins = dask.array.histogram(img, bins=2**8, range=[0, 2**8])\n",
    "#         plt.semilogy(numpy.log(histogram), label='Gray value histogram')\n",
    "#         plt.semilogy(histogram, label='Gray value histogram (log)')\n",
    "#         for c, threshold in enumerate(thresholds):\n",
    "#             plt.axvline(threshold, label='Threshold %s at %0.0f' % (c,  threshold),\n",
    "#                         color=seaborn.color_palette(n_colors=number_of_clusters)[c])\n",
    "#         plt.legend()\n",
    "#         plt.xlim([0,2**8])\n",
    "#         plt.title('Logarithmic histogram of input image with %s cluster centers' % number_of_clusters)\n",
    "#         plt.show()\n",
    "#     return(sorted(thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the MultiOtsu clustering as compared to tke k-means clustering is more robust, so we prefer that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset = 5\n",
    "# discardgrayvalue = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data['HistogramCentersKmeans'] = [histogramclustererKmeans(rec[::subset,::subset,::subset][rec[::subset,::subset,::subset] > discardgrayvalue].compute(),\n",
    "#                                                           number_of_clusters=5) for rec in ReconstructionsCrop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c, row in Data.iterrows():\n",
    "#     print(row.Sample, row.HistogramCentersKmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data['HistogramCentersMultiOtsu'] = [histogramclustererOtsu(rec[rec > discardgrayvalue],\n",
    "#                                                             number_of_clusters=5) for rec in ReconstructionsCrop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, row in Data.iterrows():\n",
    "    print(row.Sample, row.HistogramCentersMultiOtsu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [numpy.ma.masked_less(cc, 13).compressed() for cc in Data['ClusterCenters']]\n",
    "# Data['ClusterCenters'] = [numpy.ma.masked_greater(cc, 999).compressed() for cc in Data['ClusterCenters']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data['Histogramlength'] = [len(h) for h in Data['Histogram']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn.lineplot(data=Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All histograms, colored per experiment\n",
    "# for c, row in Data.iterrows():\n",
    "#     color = 0\n",
    "#     if row.Experiment == '5268':\n",
    "#         color = 1\n",
    "#     plt.semilogy(row.Histogram,\n",
    "#                  label=row.Sample,\n",
    "#                  color=seaborn.color_palette(n_colors=2)[color])\n",
    "# plt.xlim([0, 2**8])\n",
    "# plt.legend()\n",
    "# plt.savefig(os.path.join(OutPutDir, 'Histograms_Experiment.png'),  transparent=True)\n",
    "# # plt.savefig(os.path.join('P:/Talks/20230904_COMULIS/images/cox7a/Histograms_Experiment.png'),  transparent=True, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator = 500\n",
    "# for c, clustercntr in enumerate(Data['ClusterCenters']):\n",
    "#     print('-----Sample %s----' % Data['Sample'][c])\n",
    "#     for imgnr, image in enumerate(ReconstructionsCrop[c][::iterator]):\n",
    "#         for d, threshold in enumerate(clustercntr):\n",
    "#             plt.subplot(1, len(clustercntr), d + 1)\n",
    "#             plt.imshow(image)\n",
    "#             plt.imshow(image<threshold, cmap='viridis', alpha=0.5)\n",
    "#             plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))\n",
    "#             plt.title('%s\\nThreshold %s' % (os.path.basename(row.Reconstructions[row.HeadCrop:row.TailCrop][::iterator][imgnr]),\n",
    "#                                             round(threshold, 2)))\n",
    "#             plt.axis('off')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply a median filter to the cropped reconstructions\n",
    "# mediansize = 3\n",
    "# ReconstructionsMedian = [dask_image.ndfilters.median_filter(rec, size=mediansize) for rec in ReconstructionsCrop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whichsample = 2\n",
    "# step = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagenumber = range(800,len(ReconstructionsCrop[whichsample]-1000),step)\n",
    "# for i, threshold in enumerate(Data['HistogramCentersMultiOtsu'][whichsample]):\n",
    "#     print(threshold)\n",
    "#     for c, imgnmb in enumerate(imagenumber):\n",
    "#         plt.subplot(lines, int(numpy.ceil(len(imagenumber) / float(lines))), c + 1)\n",
    "#         plt.imshow(ReconstructionsCrop[whichsample][imgnmb]>threshold)\n",
    "#         plt.title('%s>%s' % (imgnmb, threshold))\n",
    "#         plt.axis('off')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out thresholded fish\n",
    "Data['OutputNameThresholded'] = [on.replace('.zarr', '.thresholded_%02d.zarr' % threshold[0]) for on,threshold in zip(Data['OutputNameRec'],\n",
    "                                                                                                                      Data['HistogramCentersMultiOtsu'])]\n",
    "for c, row in Data.iterrows():\n",
    "    if not os.path.exists(row['OutputNameThresholded']):\n",
    "        (ReconstructionsCrop[c]>row['HistogramCentersMultiOtsu'][0]).rechunk(chunks='auto').to_zarr(row['OutputNameThresholded']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load thresholded reconstructions back in\n",
    "ReconstructionsThresholded = [dask.array.from_zarr(file) for file in Data['OutputNameThresholded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write out largest component as png slices\n",
    "# # Prepare foldername first\n",
    "# # Now calculate the segmentation and write them out\n",
    "# for c, row in Data.iterrows():\n",
    "#     # Generate output folder\n",
    "#     outputdir = row.OutputNameThresholded.replace('.zarr', '')\n",
    "#     # print(outputdir)\n",
    "#     os.makedirs(outputdir, exist_ok=True)\n",
    "#     for d, name in enumerate(row.Reconstructions[row.HeadCrop:row.TailCrop]):\n",
    "#         filename = os.path.join(outputdir,\n",
    "#                                 os.path.basename(name).replace('.png', '_thresholded%02d.png' % row.HistogramCentersMultiOtsu[0]))\n",
    "#         if not os.path.exists(filename):\n",
    "#             # print(filename)\n",
    "#             imageio.imwrite(filename, ReconstructionsThresholded[c][d].astype('uint8') * 2**8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getLargestCC(segmentation):\n",
    "#     # https://stackoverflow.com/a/55110923/323100 adapted to dask\n",
    "#     labels, numfeatures = dask_image.ndmeasure.label(segmentation)\n",
    "#     labels.compute_chunk_sizes() \n",
    "#     # labels.compute()\n",
    "#     # assert( labels.max() != 0 ) # assume at least 1 CC\n",
    "#     largestCC = labels == dask.array.argmax(dask.array.bincount(dask.array.ravel(labels))[1:])+1\n",
    "#     return largestCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLargestCC(segmentation):\n",
    "    labels = skimage.measure.label(segmentation)\n",
    "    assert( labels.max() != 0 ) # assume at least 1 CC\n",
    "    largestCC = labels == numpy.argmax(numpy.bincount(labels.flat)[1:]) + 1\n",
    "    return largestCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out largest component of thresholded fish\n",
    "# Save out as NUMPY file, because we couldn't get dask to work properly...\n",
    "Data['OutputNameLargestCC'] = [on.replace('.zarr', '.largestCC.npy') for on in Data['OutputNameThresholded']]\n",
    "for c, row in Data.iterrows():\n",
    "    if not os.path.exists(row['OutputNameLargestCC']):\n",
    "        numpy.save(row['OutputNameLargestCC'], getLargestCC(ReconstructionsThresholded[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load thresholded reconstructions back in\n",
    "ReconstructionsLargestCC = [numpy.load(file) for file in Data['OutputNameLargestCC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in Data['OutputNameLargestCC']:\n",
    "#    # https://stackoverflow.com/a/72666271/323100\n",
    "#    print(file.replace('.npy','.zarr'))\n",
    "#    arr = numpy.load(file, mmap_mode='r')\n",
    "#    da = dask.array.from_array(arr).rechunk(chunks='auto')\n",
    "#    da.to_zarr(file.replace('.npy','.zarr'),\n",
    "#               overwrite=True,\n",
    "#               compressor=Blosc(cname='zstd',\n",
    "#                                shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for whichsample in range(len(Data)):\n",
    "#     whichslice=500\n",
    "#     plt.subplot(131)\n",
    "#     plt.imshow(ReconstructionsCrop[whichsample][whichslice])\n",
    "#     plt.title('%s, original' % Data.Sample[whichsample])\n",
    "#     plt.axis('off')\n",
    "#     plt.subplot(132)\n",
    "#     plt.imshow(ReconstructionsThresholded[whichsample][whichslice])\n",
    "#     plt.title('thresholded with %s' % Data['HistogramCentersMultiOtsu'][whichsample][0])\n",
    "#     plt.axis('off')\n",
    "#     plt.subplot(133)\n",
    "#     plt.imshow(ReconstructionsLargestCC[whichsample][whichslice])\n",
    "#     plt.title('slice %s, largest CC' % whichslice)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data['ClusterCentersMedian'] =[histogramclustererKmeans(rec[::subset,::subset,::subset][rec[::subset,::subset,::subset] > discardgrayvalue]) for rec in ReconstructionsMedian]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for c, row in Data.iterrows():\n",
    "#    print(row.Sample, row.ClusterCentersMedian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data['ClusterCentersMedian'] = [numpy.ma.masked_less_equal(ccm, 15).compressed() for ccm in Data['ClusterCentersMedian']]\n",
    "# Data['ClusterCentersMedian'] = [numpy.ma.masked_greater(ccm, 60).compressed() for ccm in Data['ClusterCentersMedian']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c, row in Data.iterrows():\n",
    "#     print(row.Sample, row.ClusterCentersMedian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of median data per experiment\n",
    "#for c, experiment in enumerate(Data.Experiment.unique()):\n",
    "#    plt.subplot(2, 1, c + 1)\n",
    "#    plt.title(experiment)\n",
    "#    for c, row in Data[Data.Experiment == experiment].iterrows():\n",
    "#        plt.semilogy(row.HistogramMedian,\n",
    "#                     label=row.Sample,\n",
    "#                     color=seaborn.color_palette(n_colors=len(Data))[c])\n",
    "#        for cc in row.ClusterCentersMedian:\n",
    "#            plt.axvline(cc,\n",
    "#                        color=seaborn.color_palette(n_colors=len(Data))[c],\n",
    "#                        alpha=.616)\n",
    "#    plt.xlim([0, 2**6])\n",
    "#    plt.legend()\n",
    "#plt.savefig(os.path.join(OutPutDir, 'Histograms.Median.Experiment.ClusterCenters.png'))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All median histograms, colored per experiment\n",
    "#for c, row in Data.iterrows():\n",
    "#    color = 0\n",
    "#    if row.Experiment == 'WT':\n",
    "#        color = 1\n",
    "#    plt.semilogy(row.HistogramMedian,\n",
    "#                 label=row.Sample,\n",
    "#                 color=seaborn.color_palette(n_colors=2)[color])\n",
    "#plt.xlim([0, 2**8])\n",
    "#plt.legend()\n",
    "#plt.savefig(os.path.join(OutPutDir, 'Histograms.Median.Experiment.png'), transparent=True)\n",
    "#plt.show(#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OutPutDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate peaks of all histograms, we use them for the segmentation afterwards\n",
    "# # If we would use '-h', we can calculate the (first) valley of the histogram, which mostly conforms to the first peak of the MultiOtsu\n",
    "# Data['Peaks'] = [scipy.signal.find_peaks(h) for h in Data['Histogram']]\n",
    "# # Mask out background and large gray values\n",
    "# Data['Peaks'] = [dask.array.ma.masked_outside(p, 1, 80).compute().compressed() for p, details in Data['Peaks']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data['Peaks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Histograms per experiment\n",
    "# for c, row in Data.iterrows():\n",
    "#     plt.subplot(131)\n",
    "#     plt.semilogy(row.Histogram, c=row.Color)\n",
    "#     for d, center in enumerate(row.HistogramCentersMultiOtsu):\n",
    "#         if not d:\n",
    "#             plt.axvline(center, c='black', label=round(center))\n",
    "#         else:\n",
    "#             plt.axvline(center, c=row.Color, label=round(center))\n",
    "#     plt.title('MultiOtsu %s' % row.Sample)\n",
    "#     plt.xlim([0,2**8])\n",
    "#     plt.legend()\n",
    "#     plt.subplot(132)\n",
    "#     plt.semilogy(row.Histogram, c=row.Color)\n",
    "#     for center in row.HistogramCentersKmeans:\n",
    "#         plt.axvline(center, c=row.Color, label=round(center))\n",
    "#     plt.title('KMeans %s' % row.Sample)\n",
    "#     plt.xlim([0,2**8])\n",
    "#     plt.legend()\n",
    "#     plt.subplot(133)\n",
    "#     plt.semilogy(row.Histogram, c=row.Color)\n",
    "#     for center in row.Peaks:\n",
    "#         plt.axvline(center, c=row.Color, label=round(center))\n",
    "#     plt.title('Peaks %s' % row.Sample)\n",
    "#     plt.xlim([0,2**8])\n",
    "#     plt.legend()\n",
    "#     outputname = os.path.join(os.path.dirname(row.Folder), '%s.%s.Histograms.png' % (row.Sample, row.RecFolder))\n",
    "#     if not os.path.exists(outputname):\n",
    "#         plt.savefig(outputname)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c, row in Data.iterrows():\n",
    "#     plt.semilogy(row.Histogram, c=row.Color, alpha=.618, label=row.Experiment)\n",
    "#     plt.axvline(row.HistogramCentersMultiOtsu[0], c=row.Color, alpha=.618)\n",
    "# legend_without_duplicate_labels(plt.gca())\n",
    "# seaborn.despine()\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(os.path.join(OutPutDir, 'grayvaluehistogram.log.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data['Volume'] = [dask.array.count_nonzero(rec > threshold[0]).compute() for rec, threshold in zip(ReconstructionsCrop,\n",
    "#                                                                                                    Data.HistogramCentersMultiOtsu)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Experiment', 'Color', 'Volume', 'VolumelCC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.boxenplot(data=Data,\n",
    "                  x='Experiment',\n",
    "                  y='VolumelCC',\n",
    "                  hue='Experiment',\n",
    "                  width=0.8,\n",
    "                  #https://stackoverflow.com/questions/68297642/how-to-choose-seaborn-boxplots-color-by-hue#comment120707639_68297642                \n",
    "                  palette={\"5268\": \"#00E6B8\", \"5269\": \"#E6002E\"},\n",
    "                  saturation=1)\n",
    "seaborn.swarmplot(data=Data,\n",
    "                  x='Experiment',\n",
    "                  y='VolumelCC',\n",
    "                  hue='Experiment',\n",
    "                  palette={\"5268\": \"#00E6B8\", \"5269\": \"#E6002E\"},                  \n",
    "                  s=10,\n",
    "                  edgecolor='black',\n",
    "                  linewidth=1,\n",
    "                  # jitter=0.8/2,\n",
    "                  # alpha=0.618\n",
    "                 )\n",
    "plt.ylim(ymin=0)\n",
    "plt.ylabel('Voxels > Threshold')\n",
    "seaborn.despine()\n",
    "plt.savefig(os.path.join(OutPutDir, 'Volume_Boxplot.5268_5269.ThresholdedVolumeLargestComponent.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data['VolumelCC'] = [numpy.count_nonzero(rlCC) for rlCC in ReconstructionsLargestCC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Sample', 'HistogramCentersMultiOtsu', 'Volume', 'VolumelCC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, row in Data.iterrows():\n",
    "    plt.semilogy(row.Histogram, c=row.Color)\n",
    "    for d,threshold in enumerate(row.HistogramCentersMultiOtsu):\n",
    "        plt.axvline(threshold, c='gray', label=threshold)\n",
    "    plt.legend(facecolor='white', framealpha=0.618)\n",
    "    plt.title(row.Sample)\n",
    "    seaborn.despine()\n",
    "    plt.savefig(os.path.join(OutPutDir, 'histogram.%s.png' % row.Sample), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Volume', 'VolumelCC', 'SegmentedVolume_mm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.boxenplot(data=Data,\n",
    "                  x='Experiment',\n",
    "                  y='SegmentedVolume_mm',\n",
    "                  hue='Experiment',\n",
    "                  width=0.8,\n",
    "                  #https://stackoverflow.com/questions/68297642/how-to-choose-seaborn-boxplots-color-by-hue#comment120707639_68297642                \n",
    "                  palette={\"5268\": \"#00E6B8\", \"5269\": \"#E6002E\"},\n",
    "                  saturation=1)\n",
    "seaborn.swarmplot(data=Data,\n",
    "                  x='Experiment',\n",
    "                  y='SegmentedVolume_mm',\n",
    "                  hue='Experiment',\n",
    "                  palette={\"5268\": \"#00E6B8\", \"5269\": \"#E6002E\"},                  \n",
    "                  s=10,\n",
    "                  edgecolor='black',\n",
    "                  linewidth=1,\n",
    "                  # jitter=0.8/2,\n",
    "                  # alpha=0.618\n",
    "                 )\n",
    "plt.ylim(ymin=0)\n",
    "plt.ylabel('Segmented volume [mm³]')\n",
    "seaborn.despine()\n",
    "plt.savefig(os.path.join(OutPutDir, 'Volume_Boxplot.5268_5269.SegmentedVolumemm3.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OutPutDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whichone = 6\n",
    "# slice = 555\n",
    "# plt.subplot(131)\n",
    "# plt.imshow(ReconstructionsCrop[whichone][slice])\n",
    "# plt.title(Data.Sample[whichone])\n",
    "# plt.subplot(132)\n",
    "# plt.imshow(ReconstructionsCrop[whichone][slice] > Data.Peaks[whichone])\n",
    "# plt.title(Data.Peaks[whichone])\n",
    "# plt.subplot(133)\n",
    "# plt.semilogy(Data.Histogram[whichone])\n",
    "# plt.axvline(Data.Peaks[whichone], label='Histogram')\n",
    "# # for c, center in enumerate(Data.ClusterCenters[whichone]):\n",
    "# #     plt.axvline(center, label='ClusterCenter %s' % c)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.ClusterCenters[whichone]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data['Volume_median'] = [dask.array.count_nonzero(rec > volumethreshold).compute() for rec in ReconstructionsMedian]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plt.rcParams['figure.figsize'] = (9, 9)  # Size up figures a bit    \n",
    "# seaborn.boxplot(data=Data,\n",
    "#                 x='Experiment',\n",
    "#                 y='Volume_median',\n",
    "#                 hue='Experiment',\n",
    "#                 saturation=1)\n",
    "# seaborn.swarmplot(data=Data,\n",
    "#                   x='Experiment',\n",
    "#                   y='Volume_median',\n",
    "#                   hue='Experiment',\n",
    "#                   s=15,\n",
    "#                   linewidth=2\n",
    "#                  )\n",
    "# for c, exp in enumerate(Data.Experiment.unique()):\n",
    "#     for d, row in Data[Data.Experiment == exp].iterrows():\n",
    "#         plt.text(c+0.05, row.Volume_median, row.Fish)\n",
    "# plt.ylabel('Number of voxels with gray value > %s' % volumethreshold)       \n",
    "# plt.ylim(ymin=0)\n",
    "# plt.title('Volume of median-filtered cropped reconstructions > %s' % volumethreshold)\n",
    "# plt.savefig(os.path.join(OutPutDir, 'Volume_Median_Boxplot.png'), transparent=True)\n",
    "# # plt.savefig(os.path.join('P:/Talks/20230904_COMULIS/images/cox7a/Volume_Boxplot.png'),  transparent=True, bbox_inches='tight')\n",
    "# plt.show()\n",
    "# # plt.rcParams['figure.figsize'] = (16, 9)  # Size up figures a bit    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in Data['ClusterCentersMedian']:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All histograms with *all* peaks\n",
    "# lines = 2\n",
    "# for c, row in Data.iterrows():\n",
    "#     plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)\n",
    "#     color = 0\n",
    "#     if row.Experiment == 'WT':\n",
    "#         color = 1\n",
    "#     plt.plot(row.Histogram,\n",
    "#              label='raw',\n",
    "#              color=seaborn.color_palette(n_colors=3)[0])\n",
    "#     plt.plot(row.HistogramMedian,\n",
    "#              label='median',\n",
    "#              color=seaborn.color_palette(n_colors=3)[1])\n",
    "#     # Plot them peaks\n",
    "#     for p in row['Peaks']:\n",
    "#         plt.axvline(p,\n",
    "#                     label=p,\n",
    "#                     color=seaborn.color_palette(n_colors=3)[2])\n",
    "#     plt.title(os.path.join(row.Sample, row.RecFolder))\n",
    "#     plt.xlim([0, 2**6])\n",
    "#     plt.ylim([0, 1.5e8])\n",
    "#     plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All histograms with *all* cluster centers\n",
    "# lines = 2\n",
    "# for c, row in Data.iterrows():\n",
    "#     plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)\n",
    "#     color = 0\n",
    "#     if row.Experiment == '5268':\n",
    "#         color = 1\n",
    "#     plt.plot(row.Histogram,\n",
    "#              label='raw',\n",
    "#              color=seaborn.color_palette(n_colors=3)[0])\n",
    "#     plt.plot(row.HistogramMedian,\n",
    "#              label='median',\n",
    "#              color=seaborn.color_palette(n_colors=3)[1])\n",
    "#     # Plot them centers\n",
    "#     for p in row['ClusterCentersMedian']:\n",
    "#         plt.axvline(p,\n",
    "#                     label='%0.2f' % p,\n",
    "#                     color=seaborn.color_palette(n_colors=3)[2])\n",
    "#     plt.xlim([0, 2**6])\n",
    "#     plt.ylim([0, 1.5e8])\n",
    "#     plt.title(os.path.join(row.Sample, row.RecFolder))    \n",
    "#     plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for whichsample in range(len(Data)):\n",
    "#     whichslice = int(round(Data.Size[whichsample][0] * 0.25))\n",
    "#     img = ReconstructionsMedian[whichsample][whichslice].compute()\n",
    "#     plt.subplot(1, len(Data.Peaks[whichsample]) + 1, 1)\n",
    "#     plt.semilogy(Data.HistogramMedian[whichsample])\n",
    "#     for c, peak in enumerate(Data.Peaks[whichsample]):\n",
    "#         plt.axvline(peak, label='Peak %s at %02.2f' % (c, peak), color='red')\n",
    "#     for c, cc in enumerate(Data.ClusterCentersMedian[whichsample]):\n",
    "#         plt.axvline(cc, label='Cluster center %s at %02.2f' % (c, cc), color='green')\n",
    "#     plt.legend()\n",
    "#     plt.xlim([0, 2**7])\n",
    "#     plt.ylim([0, 2e8])\n",
    "#     for c, peak in enumerate(Data.Peaks[whichsample]):\n",
    "#         plt.subplot(1, len(Data.Peaks[whichsample]) + 1, c + 2)\n",
    "#         plt.imshow(img)\n",
    "#         plt.imshow(dask.array.ma.masked_outside(img, peak - 5, peak + 5), cmap='viridis')\n",
    "#         plt.title(Data.Peaks[whichsample][c])\n",
    "#     plt.suptitle('Peaks of median histogram: %s, slice %s' % (os.path.join(Data.Sample[whichsample], Data.RecFolder[whichsample]), whichslice))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for whichsample in range(len(Data)):\n",
    "#     whichslice = int(round(Data.Size[whichsample][0] * 0.25))\n",
    "#     img = ReconstructionsMedian[whichsample][whichslice].compute()\n",
    "#     plt.subplot(1, len(Data.ClusterCentersMedian[whichsample]) + 1, 1)\n",
    "#     plt.semilogy(Data.HistogramMedian[whichsample])\n",
    "#     for c, peak in enumerate(Data.Peaks[whichsample]):\n",
    "#         plt.axvline(peak, label='Peak %s at %02.2f' % (c, peak), color='red')\n",
    "#     for c, cc in enumerate(Data.ClusterCentersMedian[whichsample]):\n",
    "#         plt.axvline(cc, label='Cluster center %s at %02.2f' % (c, cc), color='green')\n",
    "#     plt.legend()\n",
    "#     plt.xlim([0, 2**7])\n",
    "#     plt.ylim([0, 2e8])\n",
    "#     for c, peak in enumerate(Data.ClusterCentersMedian[whichsample]):\n",
    "#         plt.subplot(1, len(Data.ClusterCentersMedian[whichsample]) + 1, c + 2)\n",
    "#         plt.imshow(img)\n",
    "#         plt.imshow(dask.array.ma.masked_outside(img, peak - 5, peak + 5), cmap='viridis')\n",
    "#         plt.title('%0.2f' % Data.ClusterCentersMedian[whichsample][c])\n",
    "#     plt.suptitle('Cluster centers of median histogram: %s, slice %s' % (os.path.join(Data.Sample[whichsample], Data.RecFolder[whichsample]), whichslice))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def segmentor(image, peaks, verbose=False):\n",
    "#     # https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_random_walker_segmentation.html#sphx-glr-auto-examples-segmentation-plot-random-walker-segmentation-py\n",
    "#     # Set the background/discarded pixels to -1, 'stuff to be segmented' to 1 and cartilage/bone to 2, according to\n",
    "#     # the details given on https://scipy-lectures.org/packages/scikit-image/index.html?highlight=random%20walker#random-walker-segmentation\n",
    "#     if verbose:\n",
    "#         print('Segmenting around %s peaks (%s)' % (len(peaks), peaks))\n",
    "#     # Set everything to unlabeled\n",
    "#     markers = numpy.zeros_like(image, dtype='uint')\n",
    "#     # Set everything below background to be discarded.\n",
    "#     markers[image <= peaks[0]] = -1\n",
    "#     # Set everything around first peak to 1, corresponding mostly to the sample holder\n",
    "#     around = 2  # This much 'around' the original gray value\n",
    "#     markers[(image > peaks[0] - around) & (image < peaks[0] + around)] = 1\n",
    "#     # Set everything around a small part of the muscle histogram peak to 2\n",
    "#     # The muscle-peak correspond to the second-to last one\n",
    "#     markers[(image > peaks[-1] - around) & (image < peaks[-1] + around)] = 2\n",
    "#     # Seed high gray values with 3, these pixels are mostly cartilage and bone\n",
    "#     markers[image > 50] = 3\n",
    "#     # Do the segmentation now\n",
    "#     labels = random_walker(image, markers, beta=0)\n",
    "#     if verbose:\n",
    "#         markers = markers.compute()\n",
    "#         plt.subplot(2, 2, 1)\n",
    "#         plt.imshow(image)\n",
    "#         plt.imshow(dask.array.ma.masked_where(-1, markers), cmap='viridis', alpha=0.5)        \n",
    "#         plt.title('Original image with marker')\n",
    "#         plt.axis('off')        \n",
    "#         for c, value in enumerate(numpy.unique(markers)):\n",
    "#             plt.subplot(2, len(numpy.unique(markers)), len(numpy.unique(markers)) + c + 1)\n",
    "#             plt.imshow(image)\n",
    "#             plt.imshow(dask.array.ma.masked_not_equal(markers, value),\n",
    "#                        cmap='viridis_r')\n",
    "#             if c == 0:\n",
    "#                 plt.title('%s=discarded' % value)\n",
    "#             elif c <= len(peaks):\n",
    "#                 plt.title('%d=%0.2f +- %0.2f' % (value, peaks[c-1], around))\n",
    "#             else:\n",
    "#                 plt.title('%d>%0.2f' % (value, 50))\n",
    "#             plt.axis('off')\n",
    "#         plt.subplot(2, 2, 2)\n",
    "#         plt.imshow(labels)\n",
    "#         plt.title('Segmentation')\n",
    "#         plt.axis('off')        \n",
    "#         plt.show()\n",
    "#     # Return labeled image as an interpolated 8bit image\n",
    "#     return(numpy.interp(labels, (labels.min(), labels.max()), (0, 255)).astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def segmentor(image, peaks, verbose=False):\n",
    "#     # https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_random_walker_segmentation.html#sphx-glr-auto-examples-segmentation-plot-random-walker-segmentation-py\n",
    "#     # Set the background/discarded pixels to -1, 'stuff to be segmented' to 1 and cartilage/bone to 2, according to\n",
    "#     # the details given on https://scipy-lectures.org/packages/scikit-image/index.html?highlight=random%20walker#random-walker-segmentation\n",
    "#     if verbose:\n",
    "#         print('Input: %s peaks (%s)' % (len(peaks), peaks))\n",
    "#     # Set everything to unlabeled\n",
    "#     markers = numpy.zeros_like(image, dtype='uint')\n",
    "#     # Set everything below first peak to discarded.\n",
    "#     markers[image < 10] = -1\n",
    "#     if verbose:\n",
    "#         print('Setting everything < 10 to be discarded')\n",
    "#     around = 5  # This much 'around' the original gray value\n",
    "#     # Label everything around first and last peak\n",
    "#     markers[(image > peaks[0] - around) & (image < peaks[0] + around)] = 1\n",
    "#     if verbose:\n",
    "#         print('Setting everything between %0.2f and %0.2f to %0.2f' % (peaks[0] - around, peaks[0] + around, 1))\n",
    "#     markers[(image > peaks[-1] - around) & (image < peaks[-1] + around)] = 2\n",
    "#     if verbose:\n",
    "#         print('Setting everything between %0.2f and %0.2f to %0.2f' % (peaks[-1] - around, peaks[-1] + around, 2))\n",
    "#     # Label all the bright stuff\n",
    "#     markers[image > 2 * peaks[-1]] = 3\n",
    "#     if verbose:\n",
    "#         print('Setting everything > %0.2f to %0.2f' % (2 * peaks[-1], 3))\n",
    "#     # Do the segmentation now\n",
    "#     # labels = markers\n",
    "#     try:\n",
    "#         labels = random_walker(image, markers, mode='bf')\n",
    "#     except ValueError:\n",
    "#         # If we cannot do a random-walker segmentation, return an empty image\n",
    "#         label = numpy.zeros_like(image)\n",
    "#     if verbose:\n",
    "#         markers = markers.compute()\n",
    "#         plt.subplot(2, 2, 1)\n",
    "#         plt.imshow(image)\n",
    "#         plt.imshow(dask.array.ma.masked_less(markers,1),\n",
    "#                    cmap='viridis',\n",
    "#                    alpha=0.5)        \n",
    "#         plt.title('Original image with marker')\n",
    "#         plt.axis('off')        \n",
    "#         for c, value in enumerate(numpy.unique(markers)):\n",
    "#             plt.subplot(2, len(numpy.unique(markers)), len(numpy.unique(markers)) + c + 1)\n",
    "#             plt.imshow(image)\n",
    "#             plt.imshow(dask.array.ma.masked_not_equal(markers, value),\n",
    "#                        cmap='viridis_r')\n",
    "#             plt.title('%s: %d' % (c, value))\n",
    "#             plt.axis('off')\n",
    "#         plt.subplot(2, 2, 2)\n",
    "#         plt.imshow(labels)\n",
    "#         plt.title('Segmentation')\n",
    "#         plt.axis('off')        \n",
    "#         plt.show()\n",
    "#     # Return labeled image as an interpolated 8bit image\n",
    "#     if verbose:\n",
    "#         print('Returning labeled image with %s, but cast to \"uint8\"' % numpy.unique(labels))\n",
    "#     return(numpy.interp(labels, (-1, 3), (0, 255)).astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whichone = 6\n",
    "# a = segmentor(ReconstructionsCrop[whichone][333],\n",
    "#               Data.HistogramCentersMultiOtsu[whichone],\n",
    "#               verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "# def imsaver(image, filename):\n",
    "#     ''' Function for parallelizing writing out images '''\n",
    "#     if not os.path.exists(filename):\n",
    "#         imageio.imwrite(filename, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/62242245/323100\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out random-walker-segmented reconstructions from cluster centers\n",
    "# Prepare foldername first\n",
    "Data['OutputNameSegmentedClusterCenters'] = [f.replace('.zarr', '.segmented.clustercenters.zarr') for f in Data['OutputNameRec']]\n",
    "# # Now calculate the segmentation and write them out\n",
    "# for c, row in Data.iterrows():\n",
    "#     # Generate output folder\n",
    "#     os.makedirs(row.OutputNameSegmentedClusterCenters.replace('.zarr', ''), exist_ok=True)\n",
    "#     # For every reconstructions, load it's median-filtered counterpart, random-walker-segment it and write it out\n",
    "#     # But only do this for the relevant filenames, e.g. those between the crops :)\n",
    "#     outputfilenames = [os.path.join(row.OutputNameSegmentedClusterCenters.replace('.zarr', ''),\n",
    "#                                     os.path.basename(name)) for name in row.Reconstructions[row.HeadCrop:row.TailCrop]]\n",
    "#     print('Saving PNG slices to %s' % os.path.dirname(outputfilenames[0]))\n",
    "#     # Hat tip to Oleksiy for providing a snippet to parallelize the PNG writing\n",
    "#     Parallel(n_jobs=6)(delayed(imsaver)(segmentor(Reconstructions[c][d], row.HistogramCentersMultiOtsu),\n",
    "#                                         outputfilenames[d]) for d, name in enumerate(row.Reconstructions[row.HeadCrop:row.TailCrop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in segmented slices again and save them to zarr files now\n",
    "# # For practicability reasons we do this the other way round than for all the others\n",
    "# for c, row in Data.iterrows():\n",
    "#     if not os.path.exists(row['OutputNameSegmentedClusterCenters']):\n",
    "#         print('%2s/%2s: Reading %s slices and saving to %s' % (c + 1,\n",
    "#                                                                len(Data),\n",
    "#                                                                row['Number of reconstructions'],\n",
    "#                                                                row['OutputNameSegmentedClusterCenters'][len(Root) + 1:]))\n",
    "#         Segmented = dask_image.imread.imread(os.path.join(row.OutputNameSegmentedClusterCenters.replace('.zarr', ''),\n",
    "#                                                           '*rec*.png'))\n",
    "#         Segmented.rechunk(chunks='auto').to_zarr(row['OutputNameSegmentedClusterCenters'],\n",
    "#                                                  overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write out random-walker-segmented median filtered reconstructions from histogram peaks\n",
    "# # Prepare foldername first\n",
    "Data['OutputNameSegmentedPeaks'] = [f.replace('.zarr', '.segmented.peaks.zarr') for f in Data['OutputNameRec']]\n",
    "# # Now calculate the segmentation and write them out\n",
    "# for c, row in Data.iterrows():\n",
    "#     # Generate output folder\n",
    "#     os.makedirs(row.OutputNameSegmentedPeaks.replace('.zarr', ''), exist_ok=True)\n",
    "#     # For every reconstructions, load it's counterpart, random-walker-segment it and write it out\n",
    "#     outputfilenames = [os.path.join(row.OutputNameSegmentedPeaks.replace('.zarr', ''),\n",
    "#                                     os.path.basename(name)) for name in row.Reconstructions[row.HeadCrop:row.TailCrop]]\n",
    "#     print('Saving PNG slices to %s' % os.path.dirname(outputfilenames[0]))    \n",
    "#     # Hat tip to Oleksiy for providing a snippet to parallelize the PNG writing\n",
    "#     Parallel(n_jobs=-1)(delayed(imsaver)(segmentor(Reconstructions[c][d], row.Peaks),\n",
    "#                                          outputfilenames[d]) for d, name in enumerate(row.Reconstructions[row.HeadCrop:row.TailCrop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in segmented slices again and save them to zarr files now\n",
    "# # For practicability reasons we do this the other way round than for all the others\n",
    "# for c, row in Data.iterrows():\n",
    "#     if not os.path.exists(row['OutputNameSegmentedPeaks']):\n",
    "#         print('%2s/%2s: Reading %s slices and saving to %s' % (c + 1,\n",
    "#                                                                len(Data),\n",
    "#                                                                row['Number of reconstructions'],\n",
    "#                                                                row['OutputNameSegmentedPeaks'][len(Root) + 1:]))\n",
    "#         Segmented = dask_image.imread.imread(os.path.join(row.OutputNameSegmentedPeaks.replace('.zarr', ''),\n",
    "#                                                           '*rec*.png'))\n",
    "#         Segmented.rechunk(chunks='auto').to_zarr(row['OutputNameSegmentedPeaks'],\n",
    "#                                                  overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['OutputNameSegmentedClusterCenters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the (segmented) slices from their zarr arrays\n",
    "Segmented = [dask.array.from_zarr(file) for file in Data['OutputNameSegmentedClusterCenters']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the (largest segmented) slices as calculated above\n",
    "Segmented_Largest = [numpy.load(file) for file in Data['OutputNameLargestCC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.Sample[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReconstructionsThresholded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whichsample = 0\n",
    "slice = 333\n",
    "plt.subplot(131)\n",
    "plt.imshow(ReconstructionsCrop[whichsample][slice])\n",
    "plt.subplot(132)\n",
    "plt.imshow(ReconstructionsThresholded[whichsample][slice])\n",
    "plt.subplot(133)\n",
    "plt.imshow(Segmented_Largest[whichsample][slice])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Segmented_Largest[0].astype('uint8').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what we did there\n",
    "whichsample = 3\n",
    "whichslice = 600\n",
    "\n",
    "# Show image\n",
    "plt.imshow(ReconstructionsThresholded[whichsample][whichslice])\n",
    "plt.imshow(dask.array.ma.masked_equal(Segmented_Largest[whichsample][whichslice], 1), alpha=0.309, cmap='viridis_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmented\n",
    "dask.array.count_nonzero(Segmented_Largest[whichsample][whichslice]).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Background\n",
    "dask.array.count_nonzero(dask.array.invert(Segmented_Largest[whichsample][whichslice])).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output counts so we can double-check\n",
    "print('Background: %s px' % dask.array.count_nonzero(dask.array.invert(Segmented_Largest[whichsample][whichslice])).compute())\n",
    "print('Segmented: %s px' % dask.array.count_nonzero(Segmented_Largest[whichsample][whichslice]).compute())\n",
    "print('Image size: %s px x %s px = '\n",
    "      '%s px - %s px segmented = '\n",
    "      '%s px background' % (Segmented_Largest[whichsample][whichslice].shape[0],\n",
    "                            Segmented_Largest[whichsample][whichslice].shape[1],\n",
    "                            Segmented_Largest[whichsample][whichslice].shape[0] * Segmented_Largest[whichsample][whichslice].shape[1],\n",
    "                            dask.array.count_nonzero(Segmented_Largest[whichsample][whichslice]).compute(),\n",
    "                            Segmented_Largest[whichsample][whichslice].shape[0] * Segmented_Largest[whichsample][whichslice].shape[1] - dask.array.count_nonzero(Segmented_Largest[whichsample][whichslice]).compute()))\n",
    "# So we can 'just' sum the masked segmented data correctly :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask everything that was *not* segmented and calculate the sum of this volume\n",
    "Data['SegmentedVolume'] = [dask.array.count_nonzero(s).compute() for s in Segmented_Largest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReconstructionsCrop[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.SizeCrop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['SegmentedVolume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['VolumeCrop'] = [x * y * z for x, y, z in Data['SizeCrop']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReconstructionsCrop[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to cut length of fishes\n",
    "Data['SegmentedVolume_normalized_vol'] = [vol_seg / vol_data for vol_seg, vol_data in zip(Data['SegmentedVolume'],\n",
    "                                                                                          Data['VolumeCrop'])]\n",
    "Data['SegmentedVolume_normalized_length'] = [vol_seg / zxy[0] for vol_seg, zxy in zip(Data['SegmentedVolume'],\n",
    "                                                                                      Data['Size'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert volumes to cubic mm\n",
    "Data['SegmentedVolume_mm'] = [vol_vx * (vs ** 3) * 1e-9\n",
    "                              for vol_vx, vs in zip(Data['SegmentedVolume'],\n",
    "                                                    Data['Voxelsize'])]\n",
    "Data['SegmentedVolume_normalized_vol_mm'] = [vol_vx * (vs ** 3) * 1e-9\n",
    "                                             for vol_vx, vs in zip(Data['SegmentedVolume_normalized_vol'],\n",
    "                                                                   Data['Voxelsize'])]\n",
    "Data['SegmentedVolume_normalized_length_mm'] = [vol_vx * (vs ** 3) * 1e-9\n",
    "                                                for vol_vx, vs in zip(Data['SegmentedVolume_normalized_length'],\n",
    "                                                                      Data['Voxelsize'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[Data.Experiment.str.contains('5268')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, exp in enumerate(Data.Experiment.unique()):\n",
    "    print(exp)\n",
    "    for d, row in Data[Data.Experiment == exp].iterrows():\n",
    "        print(d, row.SegmentedVolume_normalized_vol_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in Data.Experiment.unique():\n",
    "    print('The %s fishes have a mean segmented volume (not normalized) of %.4f mm³'\n",
    "          % (experiment, Data[Data.Experiment == experiment]['SegmentedVolume_normalized_length_mm'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell us which fish is the median one\n",
    "# https://stackoverflow.com/a/61047899/323100\n",
    "for experiment in Data.Experiment.unique():\n",
    "    print('The median fish of the %s fishes is fish %s (df index %s) and has a volume (normalized to length) of %.4f mm³'\n",
    "          % (experiment,\n",
    "             Data[Data['SegmentedVolume_normalized_length_mm'] == Data[Data.Experiment == experiment]['SegmentedVolume_normalized_length_mm'].median()].iloc[0]['Sample'],\n",
    "             Data[Data['SegmentedVolume_normalized_length_mm'] == Data[Data.Experiment == experiment]['SegmentedVolume_normalized_length_mm'].median()].index[0],\n",
    "             Data[Data.Experiment == experiment]['SegmentedVolume_normalized_length_mm'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Experiment', 'Sample',\n",
    "      'SegmentedVolume_normalized_vol', 'SegmentedVolume_normalized_length',\n",
    "      'SegmentedVolume_mm', 'SegmentedVolume_normalized_vol_mm', 'SegmentedVolume_normalized_length_mm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All %s fishes have a mean segmented volume of %0.2f mm³'\n",
    "      % (len(Data), Data['SegmentedVolume_mm'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write XLS sheet for Carolina\n",
    "Output = Data[['Sample', 'Folder', 'LogFile', 'Experiment', 'Fish',\n",
    "               'Voxelsize', 'Number of reconstructions', 'OutputNameRec',\n",
    "               'Size', 'HeadCrop', 'TailCrop', 'SizeCrop', 'VolumeCrop',\n",
    "               'Peaks',\n",
    "               'SegmentedVolume', 'SegmentedVolume_normalized_vol', 'SegmentedVolume_normalized_length',\n",
    "               'SegmentedVolume_mm', 'SegmentedVolume_normalized_vol_mm', 'SegmentedVolume_normalized_length_mm']]\n",
    "Output.to_excel(os.path.join(Root, 'Data.5268_5269.xlsx'))\n",
    "Output.to_excel(os.path.join(OutPutDir, 'Data.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write CSV for manuscript\n",
    "Data[['Sample', 'Folder', 'LogFile', 'Experiment', 'Fish',\n",
    "      'Voxelsize', 'Number of reconstructions', 'OutputNameRec',\n",
    "      'Size', 'HeadCrop', 'TailCrop', 'SizeCrop', # 'VolumeCrop',\n",
    "      # 'Peaks',\n",
    "      # 'SegmentedVolume', 'SegmentedVolume_normalized_vol', 'SegmentedVolume_normalized_length',\n",
    "      # 'SegmentedVolume_mm', 'SegmentedVolume_normalized_vol_mm', 'SegmentedVolume_normalized_length_mm'\n",
    "     ]].to_csv(os.path.join('data', 'data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saved all the asked data to %s' % OutPutDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
